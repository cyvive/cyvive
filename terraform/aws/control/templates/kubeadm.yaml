apiVersion: kubeadm.k8s.io/v1alpha3
kind: InitConfiguration
bootstrapTokens:
- description: "cyvive controlled & shared node registration token"
  groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: ${token_id}
  ttl: '0'
  usages:
  - signing
  - authentication
nodeRegistration:
  criSocket: /run/containerd/containerd.sock
  # criSocket: unix:///run/containerd/containerd.sock
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
# apiEndpoint:
  # advertiseAddress: 192.168.0.102
  # bindPort: 6443
---
apiVersion: kubeadm.k8s.io/v1alpha3
kind: ClusterConfiguration
etcd:
  external:
    endpoints:
      - https://etcda-${cluster_fqdn}:2379
      - https://etcdb-${cluster_fqdn}:2379
      - https://etcdc-${cluster_fqdn}:2379
    caFile: /etc/kubernetes/pki/etcd/ca.crt
    certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
    keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
networking:
  dnsDomain: ${cluster_domain}
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/16
kubernetesVersion: ${kubernetes_version}
controlPlaneEndpoint: ${cluster_fqdn}
apiServerCertSANs:
  - "${cluster_fqdn}"
  - "${debug_subdomain}${cluster_fqdn}"
certificatesDir: /etc/kubernetes/pki
imageRepository: k8s.gcr.io
unifiedControlPlaneImage: ""
auditPolicy:
  path: ''
  logDir: /var/log/kubernetes/audit
  logMaxAge: 7
clusterName: kubernetes
# featureGates:
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
# Any of the Kublet configuration settings can be specified below
# https://github.com/kubernetes/kubernetes/blob/release-1.11/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go
# >>> HERE <<< cgroup  seems to be conflicting, pehaps should bind sys/cgroup back to the host instead of just the k8's container Reserved settings should also be investigated (possibly too low for initial boot)
cgroupRoot: "/"
systemReservedCgroup: systemreserved
kubeReservedCgroup: podruntime
kubeletCgroups: /podruntime/kubelet
systemCgroups: /podruntime/runtime
cgroupDriver: cgroupfs
cgroupsPerQOS: true
authentication:
  anonymous:
    # @debug Required for initial bootstrap node to use load-balancer
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
clusterDNS:
  - 10.96.0.10
clusterDomain: ${cluster_domain}
contentType: application/vnd.kubernetes.protobuf
cpuManagerPolicy: static
systemReserved:
 cpu: 500m
 memory: 400M
 # TODO assess kubelet requirements per instance type / enable dynamic reservation allocation
kubeReserved:
  cpu: 500m
  memory: 512M
# Not ready yet
# qosReserved: "memory="
enableControllerAttachDetach: true
enableDebuggingHandlers: true
enforceNodeAllocatable:
  - pods
  - system-reserved
  - kube-reserved
eventBurst: 10
eventRecordQPS: 5
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
evictionSoft:
  memory.available:  "200Mi"
  nodefs.available:  "20%"
  nodefs.inodesFree: "10%"
  imagefs.available: "30%"
evictionSoftGracePeriod:
  memory.available:  "30s"
  nodefs.available:  "30s"
  nodefs.inodesFree: "30s"
  imagefs.available: "30s"
# TODO verify that swap can be enabled
failSwapOn: false
fileCheckFrequency: 20s
hairpinMode: hairpin-veth
kubeAPIBurst: 10
kubeAPIQPS: 5
maxOpenFiles: 1000000
# https://groups.google.com/forum/#!msg/docker-user/k5hqpNg8gwQ/-00mvrB2nIkJ <= maximum networking limits per central tree span
maxPods: 960
# TODO check for optimized number
podPidsLimit: -1
podsPerCore: 100
registryBurst: 10
registryPullQPS: 5
rotateCertificates: true
serializeImagePulls: false
staticPodPath: /etc/kubernetes/manifests
syncFrequency: 1m0s
featureGates:
  SupportPodPidsLimit: true
  PodShareProcessNamespace: true
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
bindAddress: 0.0.0.0
clientConnection:
  # acceptContentTypes: ""
  burst: 10
  kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
  qps: 5
# Hard Coded to ensure compatibility with Flannel
clusterCIDR: "10.244.0.0/16"
configSyncPeriod: 15m0s
conntrack:
  max: null
  maxPerCore: 32768
  min: 131072
  tcpCloseWaitTimeout: 1h0m0s
  tcpEstablishedTimeout: 24h0m0s
enableProfiling: false
healthzBindAddress: 0.0.0.0:10256
# hostnameOverride: ""
iptables:
  masqueradeAll: false
  masqueradeBit: 14
  minSyncPeriod: 0s
  syncPeriod: 30s
ipvs:
  ExcludeCIDRs: null
  minSyncPeriod: 0s
  scheduler: ""
  syncPeriod: 30s
metricsBindAddress: 127.0.0.1:10249
mode: ""
nodePortAddresses: null
oomScoreAdj: -999
portRange: ""
resourceContainer: /kube-proxy
udpIdleTimeout: 250ms
